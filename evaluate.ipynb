{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics import FrechetInceptionDistance, KernelInceptionDistance, IPR\n",
    "\n",
    "from dataset import init_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = \"/datatmp/users/igeorvasilis/ddpm-continual-learning/results/cifar10/mask_v1_re/0\"\n",
    "\n",
    "args_dir = os.path.join(experiment_dir, \"args.pt\")\n",
    "args = torch.load(args_dir)\n",
    "args.device = 'cuda:2'\n",
    "args.dataset_path = \"/datatmp/users/igeorvasilis/datasets/cifar10\"\n",
    "\n",
    "args.pretrained_model_dir = os.path.join(experiment_dir, \"epoch-200\")\n",
    "args.train_log_dir = os.path.join(experiment_dir, \"train_log.pt\")\n",
    "args.folder_name = \"ddim_fake_images\"\n",
    "\n",
    "train_log = torch.load(args.train_log_dir)\n",
    "\n",
    "torch.manual_seed(args.gen_seed)\n",
    "np.random.seed(args.gen_seed)\n",
    "random.seed(args.gen_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_class_name': 'UNet2DModel',\n",
       " '_diffusers_version': '0.23.0',\n",
       " 'act_fn': 'silu',\n",
       " 'add_attention': True,\n",
       " 'attention_head_dim': 8,\n",
       " 'attn_norm_num_groups': None,\n",
       " 'block_out_channels': [128, 128, 256, 256, 512, 512],\n",
       " 'center_input_sample': False,\n",
       " 'class_embed_type': None,\n",
       " 'down_block_types': ['DownBlock2D',\n",
       "  'DownBlock2D',\n",
       "  'DownBlock2D',\n",
       "  'DownBlock2D',\n",
       "  'AttnDownBlock2D',\n",
       "  'DownBlock2D'],\n",
       " 'downsample_padding': 1,\n",
       " 'downsample_type': 'conv',\n",
       " 'dropout': 0.0,\n",
       " 'flip_sin_to_cos': True,\n",
       " 'freq_shift': 0,\n",
       " 'in_channels': 3,\n",
       " 'layers_per_block': 2,\n",
       " 'mid_block_scale_factor': 1,\n",
       " 'norm_eps': 1e-05,\n",
       " 'norm_num_groups': 32,\n",
       " 'num_class_embeds': None,\n",
       " 'num_train_timesteps': None,\n",
       " 'out_channels': 3,\n",
       " 'resnet_time_scale_shift': 'default',\n",
       " 'sample_size': 32,\n",
       " 'time_embedding_type': 'positional',\n",
       " 'up_block_types': ['UpBlock2D',\n",
       "  'AttnUpBlock2D',\n",
       "  'UpBlock2D',\n",
       "  'UpBlock2D',\n",
       "  'UpBlock2D',\n",
       "  'UpBlock2D'],\n",
       " 'upsample_type': 'conv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained model\n",
    "import json\n",
    "\n",
    "with open(f\"{args.pretrained_model_dir}/unet/config.json\") as f:\n",
    "    # load json, 'r') as f:\n",
    "    config = json.load(f)\n",
    "config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments:\n",
      "\tdevice: cuda:2\n",
      "\tdataset_name: CIFAR10\n",
      "\tdataset_path: /datatmp/users/igeorvasilis/datasets/cifar10\n",
      "\ttarget_dir: ./dataset\n",
      "\tpr_flip: False\n",
      "\tpr_rotate: False\n",
      "\tlabels: [0]\n",
      "\tnum_train_timesteps: 1000\n",
      "\tbeta_start: 0.0001\n",
      "\tbeta_end: 0.02\n",
      "\tbeta_schedule: squaredcos_cap_v2\n",
      "\tmask: None\n",
      "\tnum_tasks: None\n",
      "\tpipeline: ddim\n",
      "\tnum_inference_steps: 50\n",
      "\timage_size: 32\n",
      "\tin_channels: 3\n",
      "\tout_channels: 3\n",
      "\tlayers_per_block: 2\n",
      "\tblock_out_channels: [128, 128, 256, 256, 512, 512]\n",
      "\tdown_block_types: ['DownBlock2D', 'DownBlock2D', 'DownBlock2D', 'DownBlock2D', 'AttnDownBlock2D', 'DownBlock2D']\n",
      "\tup_block_types: ['UpBlock2D', 'AttnUpBlock2D', 'UpBlock2D', 'UpBlock2D', 'UpBlock2D', 'UpBlock2D']\n",
      "\tnum_epochs: 200\n",
      "\ttrain_batch_size: 64\n",
      "\teval_batch_size: 64\n",
      "\tgradient_accumulation_steps: 1\n",
      "\tlearning_rate: 0.0001\n",
      "\tlr_warmup_steps: 5\n",
      "\tpretrained_model_dir: /datatmp/users/igeorvasilis/ddpm-continual-learning/results/cifar10/mask_v1_re/0/epoch-200\n",
      "\tsample_image_epochs: 1\n",
      "\tgenerate_image_epochs: 200\n",
      "\tn_fake_images: 10000\n",
      "\tsave_model_epochs: 100\n",
      "\toutput_dir: /datatmp/users/igeorvasilis/ddpm-continual-learning/results/cifar10/mask_v1_re/0\n",
      "\tmixed_precision: fp16\n",
      "\tshow_gen_progress: False\n",
      "\tgen_seed: 0\n",
      "\ttrain_log_dir: /datatmp/users/igeorvasilis/ddpm-continual-learning/results/cifar10/mask_v1_re/0/train_log.pt\n",
      "\tfolder_name: ddim_fake_images\n"
     ]
    }
   ],
   "source": [
    "# print args values\n",
    "print(\"Arguments:\")\n",
    "for k, v in vars(args).items():\n",
    "    print(f\"\\t{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frechet Inception Distance (FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.labels = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([ transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 5000, Number of test images: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b74c059e4e4eb698aacc70c162d717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating FID for 5000 images...:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(116.8470, device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "# List all fake images from the corresponding directory \n",
    "fake_images_dir = f'{args.pretrained_model_dir}/{args.folder_name}'\n",
    "fake_images_list = os.listdir(fake_images_dir)\n",
    "n_fake_images = len(fake_images_list)\n",
    "\n",
    "# Create eval dataloader\n",
    "trainset, testset = init_dataset(dataset_name=args.dataset_name, dataset_path=args.dataset_path, \n",
    "                                 labels=args.labels, preprocess=transforms.Compose([ transforms.ToTensor()]))\n",
    "\n",
    "# trainset = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args.eval_batch_size, shuffle=True)\n",
    "\n",
    "# Define FID metric\n",
    "fid = FrechetInceptionDistance(feature=2048, normalize=True).to(args.device)\n",
    "\n",
    "# Iterate over all fake images\n",
    "n_images_to_eval = min(len(fake_images_list), len(eval_dataloader.dataset))\n",
    "for batch_idx in tqdm(range(0, n_images_to_eval, args.eval_batch_size), \n",
    "                      desc=f'Calculating FID for {n_images_to_eval} images...'):\n",
    "    \n",
    "    # Get the real images\n",
    "    real_images, _ = next(iter(eval_dataloader))\n",
    "    real_images = real_images.to(args.device)\n",
    "\n",
    "    # Get the fake images\n",
    "    fake_images = [read_image(f\"{fake_images_dir}/{i}\") for i in fake_images_list[batch_idx:batch_idx+args.eval_batch_size]]\n",
    "    fake_images = torch.stack(fake_images).to(args.device) \n",
    "    fake_images = fake_images.float() / 255.0\n",
    "    \n",
    "    real_images = real_images[:fake_images.shape[0]]\n",
    "\n",
    "    # Update the FID metric\n",
    "    fid.update(real_images, real=True)\n",
    "    fid.update(fake_images, real=False)\n",
    "\n",
    "# Compute the FID score\n",
    "print(fid.compute())\n",
    "fid.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Inception Distance (KID) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 5000, Number of test images: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0063acc5ed1e4fe78b0cb06f8b4e3982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating kid...:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.1294, device='cuda:2'), tensor(0.0016, device='cuda:2'))\n"
     ]
    }
   ],
   "source": [
    "# List all fake images from the corresponding directory \n",
    "fake_images_dir = f'{args.pretrained_model_dir}/{args.folder_name}'\n",
    "fake_images_list = os.listdir(fake_images_dir)\n",
    "n_fake_images = len(fake_images_list)\n",
    "\n",
    "# Create eval dataloader\n",
    "trainset, testset = init_dataset(dataset_name=args.dataset_name, dataset_path=args.dataset_path, \n",
    "                                 labels=args.labels, preprocess=transforms.Compose([ transforms.ToTensor()]))\n",
    "\n",
    "# trainset = torch.utils.data.ConcatDataset([trainset, testset])\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args.eval_batch_size, shuffle=True)\n",
    "\n",
    "# Define kid metric\n",
    "subsets = 100\n",
    "subset_size = 1000\n",
    "kid = KernelInceptionDistance(\n",
    "    feature=2048, \n",
    "    normalize=True,\n",
    "    subsets=subsets,\n",
    "    subset_size=subset_size\n",
    ").to(args.device)\n",
    "\n",
    "# Iterate over all fake images\n",
    "n_images_to_eval = min(len(fake_images_list), len(eval_dataloader.dataset))\n",
    "for batch_idx in tqdm(range(0, n_images_to_eval, args.eval_batch_size), desc='Calculating kid...'):\n",
    "    \n",
    "    # Get the corresponding real images\n",
    "    real_images, _ = next(iter(eval_dataloader))\n",
    "    real_images = real_images.to(args.device)\n",
    "\n",
    "    # Get the fake images\n",
    "    fake_images = [read_image(f\"{fake_images_dir}/{i}\") for i in fake_images_list[batch_idx:batch_idx+args.eval_batch_size]]\n",
    "    fake_images = torch.stack(fake_images).to(args.device) \n",
    "    fake_images = fake_images.float() / 255.0\n",
    "\n",
    "    # Update the kid metric\n",
    "    kid.update(real_images, real=True)\n",
    "    kid.update(fake_images, real=False)\n",
    "\n",
    "# Compute the kid score\n",
    "print(kid.compute())\n",
    "kid.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 10000, Number of test images: 2000\n"
     ]
    }
   ],
   "source": [
    "trainset, _ = init_dataset(dataset_name=args.dataset_name, dataset_path=args.dataset_path, labels=args.labels, preprocess=preprocess)\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args.eval_batch_size, shuffle=True)\n",
    "\n",
    "real_images = torch.cat([image for image, _ in eval_dataloader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all fake images from the corresponding directory \n",
    "fake_images_dir = f'{args.pretrained_model_dir}/{args.folder_name}'\n",
    "fake_images_list = os.listdir(fake_images_dir)\n",
    "fake_images = torch.stack([read_image(f'{fake_images_dir}/{i}') for i in fake_images_list])\n",
    "fake_images = fake_images.float() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images_to_eval = min(real_images.shape[0], fake_images.shape[0])\n",
    "real_images = real_images[:n_images_to_eval]\n",
    "fake_images = fake_images[:n_images_to_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vgg16 for improved precision and recall...done\n",
      "IPR: resizing (32, 32) to (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting features of 10000 images: 100%|██████████| 1250/1250 [00:33<00:00, 37.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from improved_precision_recall import IPR \n",
    "\n",
    "# Define IPR metric\n",
    "ipr = IPR(batch_size=8, k=3, num_samples=n_images_to_eval, device=args.device)\n",
    "\n",
    "# Compute Manifold \n",
    "ipr.compute_manifold_ref(real_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPR: resizing (32, 32) to (224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting features of 10000 images: 100%|██████████| 1250/1250 [00:33<00:00, 37.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 1 negative diff_squares found and set to zero, min_diff_square= -9.094947017729282e-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing precision...: 100%|██████████| 10000/10000 [00:00<00:00, 29049.32it/s]\n",
      "computing recall...: 100%|██████████| 10000/10000 [00:00<00:00, 28971.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.8955\n",
      "recall = 0.4318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric = ipr.precision_and_recall(fake_images)\n",
    "# Print results\n",
    "print('precision =', metric.precision)\n",
    "print('recall =', metric.recall)\n",
    "\n",
    "# r_score = ipr.realism(fake_images)\n",
    "# print('realism =', r_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc93020eaaa149f4b5293b12e8e0754d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies: {0: 0.4222244408945687, 1: 0.41902955271565495, 'no': 0.15874600638977635}\n",
      "Entropy: 1.020691376559435\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForImageClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from utils import get_preprocess_function\n",
    "args.device = 'cuda:1'\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"nateraw/vit-base-patch16-224-cifar10\").to(args.device)\n",
    "preprocess = get_preprocess_function(\"EvalCifar10\")\n",
    "\n",
    "pred_labels = {label:0 for label in args.labels}\n",
    "pred_labels['no'] = 0\n",
    "\n",
    "for b_idx in tqdm(range(0, len(fake_images_list), args.eval_batch_size)):\n",
    "    \n",
    "    fake_images_names = fake_images_list[b_idx:b_idx+args.eval_batch_size]\n",
    "    fake_images = [ read_image(f'{fake_images_dir}/{fi_name}') for fi_name in fake_images_names ]\n",
    "    fake_images = [ preprocess(image) for image in fake_images ]\n",
    "    fake_images = torch.stack(fake_images)\n",
    "    fake_images = fake_images.float() / 255.0\n",
    "\n",
    "    # Get predictions\n",
    "    outputs = model(fake_images.to(args.device))\n",
    "    outputs = torch.softmax(outputs.logits, dim=1)\n",
    "    predicted = torch.argmax(outputs, dim=1).cpu().numpy()    \n",
    "    for label in predicted: \n",
    "        if label in pred_labels: pred_labels[label] += 1\n",
    "        else: pred_labels['no'] += 1\n",
    "\n",
    "# Compute the total count of all labels\n",
    "total_count = sum(pred_labels.values())\n",
    "# Compute the frequency of each label\n",
    "frequencies = {label: count / total_count for label, count in pred_labels.items()}\n",
    "print(\"Frequencies:\", frequencies)\n",
    "\n",
    "# Compute the entropy\n",
    "entropy = -sum([p * math.log(p) for p in frequencies.values()])\n",
    "print(\"Entropy:\", entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import models\n",
    "# import torch.nn as nn\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "# import math\n",
    "\n",
    "# checkpoint = torch.load('/datatmp/users/igeorvasilis/ddpm-continual-learning/results/mnist/eval_classifier/checkpoint_1n2.pth')\n",
    "\n",
    "# # Load the actual and the opposite mapping\n",
    "# map_labels = checkpoint['mapping']\n",
    "# map_labels_inv = {v: k for k, v in map_labels.items()}\n",
    "\n",
    "# # Define the pretrained model\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# model.fc = nn.Linear(model.fc.in_features, len(args.labels))\n",
    "\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# model = model.to(args.device)\n",
    "\n",
    "# # List all fake images from the corresponding directory \n",
    "# fake_images_dir = f'{args.pretrained_model_dir}/{args.folder_name}'\n",
    "# fake_images_list = os.listdir(fake_images_dir)\n",
    "\n",
    "# n_counts = {label:0 for label in map_labels.keys()}\n",
    "\n",
    "# for b_idx in tqdm(range(0, len(fake_images_list), args.eval_batch_size)):\n",
    "    \n",
    "#     fake_images_names = fake_images_list[b_idx:b_idx+args.eval_batch_size]\n",
    "#     fake_images = [ read_image(f'{fake_images_dir}/{fi_name}') for fi_name in fake_images_names ]\n",
    "#     fake_images = torch.stack(fake_images)\n",
    "#     fake_images = fake_images.float() / 255.0\n",
    "\n",
    "#     # Get predictions\n",
    "#     outputs = model(fake_images.to(args.device))\n",
    "#     _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "#     predicted = [map_labels_inv[label.item()] for label in predicted]\n",
    "#     for label in predicted: n_counts[label] += 1\n",
    "\n",
    "# # Compute the total count of all labels\n",
    "# total_count = sum(n_counts.values())\n",
    "# # Compute the frequency of each label\n",
    "# frequencies = {label: count / total_count for label, count in n_counts.items()}\n",
    "# print(\"Frequencies:\", frequencies)\n",
    "\n",
    "# # Compute the entropy\n",
    "# entropy = -sum([p * math.log(p) for p in frequencies.values()])\n",
    "# print(\"Entropy:\", entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "fakes = [transforms.ToPILImage()(image) for image in fake_images[:32]]\n",
    "grid = make_image_grid(fakes, rows=4, cols=8)\n",
    "axes[0].imshow(grid); axes[0].set_title(f'Fake images')\n",
    "axes[0].axis('off')\n",
    "\n",
    "reals = [transforms.ToPILImage()(image) for image in real_images[:32]]\n",
    "grid = make_image_grid(reals, rows=4, cols=8)\n",
    "axes[1].imshow(grid); axes[1].set_title(f'Real images')\n",
    "axes[1].axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
